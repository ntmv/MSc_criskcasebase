---
title: "Relaxed Lasso Glmnet Simulation"
author: "Alexander Romanus"
date: "`r Sys.Date()`"
output:
  html_document: 
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
library(casebase)
library(future.apply)
library(glmnet)
#library(mtool)
library(parallelly)
library(timereg)
library(parallel)
library(tictoc)
library(tidyverse)
#library(riskRegression)
library(cmprsk)
library(survsim)
library(caret)
library(Matrix)
#install.packages(c("matrixCalc"))
install.packages( "/Users/alex/Downloads/lmvar_1.5.2.tar.gz", repos=NULL, type="source")
library(lmvar)

# Fitting functions 
#source("src/fitting_functions.R")
```

``` {r generate-competing-risks-data, eval=FALSE, echo=FALSE, include = FALSE}
n <- 400
p <- 20
beta <- list(c(0.5, rep(0, 18), 0.5),c(0.2, rep(0, 18), 0.2))
dist.ev <- c("weibull", "weibull")
anc.ev <- c(0.8, 0.3)
beta0.ev <- c(0.1, 0.1)


# Generating survival data 
# This function is modified from the survsim package and can be found in the fitting_functions.R file sourced above
#
sim.data <- crisk.sim_mvn(n = n, p = p, rho = 0.5, foltime = 4, dist.ev = dist.ev, 
                      anc.ev = anc.ev, beta0.ev = beta0.ev, beta0.cens = 0.05, anc.cens = 4, nsit = 2, 
                      beta = beta)

# fix status variable
sim.data$cause <- with(sim.data, ifelse(is.na(sim.data$cause), 0, sim.data$cause))
colnames(sim.data)[grepl("x", colnames(sim.data))]   <- paste0("X", seq_len(p))

# Format data
sim.data <- sim.data %>%
  select(-nid, -status, -start, -stop, -z) %>%
  rename(status = cause)

# Average estimates of incidence and censoring rate
prop.table(table(sim.data$status))

# True cumulative incidence 
cif <- cuminc(ftime = sim.data$time, fstatus = sim.data$status)
cif
```

``` {r train-test-split-and-formatting, eval=FALSE, echo=FALSE}
#################################################################
# Split into training and validation sets (stratified)
train.index <- caret::createDataPartition(sim.data$status, p = 0.80, list = FALSE)
train <- sim.data[train.index,]
test <- sim.data[-train.index,]
######################### Cause-specific proportional hazards model ###############
# Censor competing event
y_train <- Surv(time = train$time, event = train$status == 1)

x_train <- model.matrix(~ . -time -status, data = train)[, -1] 

# Censor competing event
y_test <- Surv(time = test$time, event = test$status == 1)

x_test <- model.matrix(~ . -time -status, data = test)[, -1]
```

``` {r generate-case-base-data, include = FALSE, eval=FALSE, echo=FALSE}
surv_obj_train <- with(train, Surv(time, as.numeric(status), type = "mstate"))

cov_train <- as.matrix(cbind(train[, c(grepl("X", colnames(train)))], time = log(train$time)))

# Create case-base dataset
cb_data_train <- create_cbDataset(surv_obj_train, cov_train, ratio = 10)
```

``` {r fit-multinomial-logistic-to-hazard, eval=FALSE, echo=FALSE, include = FALSE}
tic()
cv.alpha <- mtool.multinom.cv(cb_data_train, lambda_max = 0.3, alpha = 1, nfold = 10)
toc()

cv.alpha
```


``` {r results, eval=FALSE, echo=FALSE, include = FALSE}
# Cross-validation plot 
p1 <- plot_cv.multinom(cv.alpha$deviance_grid, cv.alpha$lambdagrid, cv.alpha$lambda.min, cv.alpha$lambda.1se, nfold = 10)

# validation set
surv_obj_val <- with(test, Surv(time, as.numeric(status), type = "mstate"))

# Covariance matrix
cov_val <- cbind(test[, c(grepl("X", colnames(test)))], time = log(test$time))

# Case-base dataset
cb_data_val <- create_cbDataset(surv_obj_val, as.matrix(cov_val))
```

``` {r relaxed-simulation-train-test-split, echo = FALSE}
data(QuickStartExample)
set.seed(2023)

train.index = caret::createDataPartition(QuickStartExample$y, p = 0.80, list = FALSE)

x_train = QuickStartExample$x[train.index,]
y_train = QuickStartExample$y[train.index,]

x_test = QuickStartExample$x[-train.index,]
y_test = QuickStartExample$y[-train.index,]
```

```{r setup-coefficient-results-table, echo = FALSE}
coefficient_names = colnames(as.data.frame(x_train))
coefficient_names = c("Model fitting procedure", coefficient_names)
coefficient_values = data.frame(matrix(nrow = 0, ncol = length(coefficient_names) + 1))
```


# Fits

### Fit glmnet with relax = TRUE

``` {r glmnet-relaxed-true-baseline, echo = FALSE}
set.seed(2023)
fit_relaxed = glmnet(x_train, y_train, gamma = 0, relax = TRUE)
cv_fit_relaxed = cv.glmnet(x_train, y_train, gamma = 0, relax = TRUE)

cv_lambda_relaxed_min = cv_fit_relaxed$relaxed$lambda.min
cv_lambda_relaxed_1se = cv_fit_relaxed$relaxed$lambda.1se
cv_lambda_min_index = cv_fit_relaxed$relaxed$index[1]
cv_lambda_1se_index = cv_fit_relaxed$relaxed$index[2]

coef_cv_fit_relaxed_1se = cv_fit_relaxed$glmnet.fit$relaxed$beta[, cv_lambda_1se_index]
coef_cv_fit_relaxed_min = cv_fit_relaxed$glmnet.fit$relaxed$beta[, cv_lambda_min_index]

non_zero_coef_relaxed_1se = coef_cv_fit_relaxed_1se[coef_cv_fit_relaxed_1se != 0]
non_zero_coef_relaxed_1se_indeces = which(coef_cv_fit_relaxed_1se %in% non_zero_coef_relaxed_1se)

non_zero_coef_relaxed_min = coef_cv_fit_relaxed_min[coef_cv_fit_relaxed_min != 0]
non_zero_coef_relaxed_min_indeces = which(coef_cv_fit_relaxed_min %in% non_zero_coef_relaxed_min)

# new_x_train = x_train[, non_zero_coef_relaxed_indeces]
# #relaxed_then_OLS = lm(y_train~new_x_train)
# 
# 
# attempt1 = cv.glmnet(x_train, y_train, family="gaussian", relax = FALSE, alpha = 1)
# lambda_min = attempt1$lambda.min
# lambda_1se = attempt1$lambda.1se



coefficient_values = rbind(coefficient_values, c(0, lapply(coef_cv_fit_relaxed_min, function(x) round(x, 3))), c(0, lapply(coef_cv_fit_relaxed_1se, function(x) round(x, 3))))
colnames(coefficient_values) = coefficient_names
```


### Fit lasso without cross validation, then OLS on all lambda values
Per meeting on June 15th, tried fitting LASSO in first iteration without cross validation, took sets of predictors selected with each lambda value tried, then fit OLS on each of those sets, taking the OLS fit with the lowest MSE as the best fit.

``` {r lasso-then-ols-all-predictors, echo = FALSE}
set.seed(2023)
fit_lasso = glmnet(x_train, y_train, family="gaussian", alpha=1)

all_coef_fit_lasso = as.data.frame.matrix(coef(fit_lasso))[-1, ]

current_MSE = .Machine$double.xmax;
best_fit = lm(1~1)
non_zero_coef_lasso_then_OLS_all_predictors_indeces_best_fit = c()

for(i in (1: length(all_coef_fit_lasso))) {
  current_coef = all_coef_fit_lasso[i]
  non_zero_coef_lasso_then_OLS_all_predictors = all_coef_fit_lasso[i][all_coef_fit_lasso[i] != 0]
  if (length(non_zero_coef_lasso_then_OLS_all_predictors) == 0){
    next
  }
  
  non_zero_coef_lasso_then_OLS_all_predictors_indeces = which(current_coef != 0)
  
  new_x_train = x_train[, non_zero_coef_lasso_then_OLS_all_predictors_indeces]
  
  data = data.frame(X = new_x_train, y = y_train)
  
  fit_OLS_on_LASSO_subset = lm(y~., data = data, x = TRUE, y = TRUE)
  
  coef_fit_OLS_on_LASSO_subset = coef(fit_OLS_on_LASSO_subset)
  
  #CHANGE THIS
  coef_fit_OLS_on_LASSO_subset = coef_fit_OLS_on_LASSO_subset[-1]

  fit_OLS_cv_on_LASSO_subset = cv.lm(fit_OLS_on_LASSO_subset, k = 5)
  MSE = fit_OLS_cv_on_LASSO_subset$MSE$mean
  
  if(MSE < current_MSE) {
    current_MSE = MSE
    best_fit = fit_OLS_on_LASSO_subset
    
    non_zero_coef_lasso_then_OLS_all_predictors_indeces_best_fit = non_zero_coef_lasso_then_OLS_all_predictors_indeces

    
    coef_lasso_then_OLS_all_predictors_final = rep(0, 20)
    j = 1
    for (l in c(1:20)) {
      if (l %in% non_zero_coef_lasso_then_OLS_all_predictors_indeces){
        coef_lasso_then_OLS_all_predictors_final[l] = coef_fit_OLS_on_LASSO_subset[j]
        j = j + 1
      }
    }
  }
}

fit_lasso_then_OLS_all_predictors = best_fit


coefficient_values = rbind(coefficient_values, c(0, lapply(coef_lasso_then_OLS_all_predictors_final, function(x) round(x, 3))))
colnames(coefficient_values) = coefficient_names
```


### Fit lasso with cross validation, then OLS on all lambda values
Same fit as above but LASSO fit in first iteration is done with cross validation

``` {r lasso-cv-then-ols-all-predictors, echo = FALSE}
set.seed(2023)
fit_lasso_cv = cv.glmnet(x_train, y_train, family="gaussian", alpha=1)

all_coef_fit_lasso_cv = as.data.frame.matrix(fit_lasso_cv$glmnet.fit$beta)

current_MSE = .Machine$double.xmax;
best_fit = lm(1~1)
non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces_best_fit = c()

for(i in (1: length(all_coef_fit_lasso_cv))) {
  current_coef_cv = all_coef_fit_lasso_cv[i]
  non_zero_coef_lasso_cv_then_OLS_all_predictors = all_coef_fit_lasso_cv[i][all_coef_fit_lasso_cv[i] != 0]
  if (length(non_zero_coef_lasso_cv_then_OLS_all_predictors) == 0){
    next
  }
  
  non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces = which(current_coef_cv != 0)
  
  new_x_train = x_train[, non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces]
  
  data = data.frame(X = new_x_train, y = y_train)
  
  fit_OLS_on_LASSO_cv_subset = lm(y~., data = data, x = TRUE, y = TRUE)
  
  coef_fit_OLS_on_LASSO_cv_subset = coef(fit_OLS_on_LASSO_cv_subset)
  
  #CHANGE THIS
  coef_fit_OLS_on_LASSO_cv_subset = coef_fit_OLS_on_LASSO_cv_subset[-1]

  fit_OLS_cv_on_LASSO_cv_subset = cv.lm(fit_OLS_on_LASSO_cv_subset, k = 5)
  MSE = fit_OLS_cv_on_LASSO_cv_subset$MSE$mean
  
  if(MSE < current_MSE) {
    current_MSE = MSE
    best_fit = fit_OLS_on_LASSO_cv_subset
    
    non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces_best_fit = non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces
    

    coef_lasso_cv_then_OLS_all_predictors_final = rep(0, 20)
    j = 1
    for (l in c(1:20)) {
      if (l %in% non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces){
        coef_lasso_cv_then_OLS_all_predictors_final[l] = coef_fit_OLS_on_LASSO_cv_subset[j]
        j = j + 1
      }
    }
  }
}

fit_lasso_cv_then_OLS_all_predictors = best_fit

coefficient_values = rbind(coefficient_values, c(0, lapply(coef_lasso_cv_then_OLS_all_predictors_final, function(x) round(x, 3))))
colnames(coefficient_values) = coefficient_names
```

# Results

## Coefficient values of fitting procedures

``` {r relaxed-lasso-glmnet-implementation-coefficient-values, echo=FALSE}
colnames(coefficient_values) = coefficient_names
model_fit_labels = c("CV, lambda min, relax = TRUE", "CV, lambda 1se, relax = TRUE", "lasso noCV relax = FALSE, then OLS fit on selected predictors from every lambda", "lasso CV relax = FALSE, then OLS fit on selected predictors from every lambda")


options(digits = 5)

coefficient_values[, 1] = model_fit_labels

coefficient_values
```


## R-squared values of fitting procedures

``` {r relaxed-lasso-glmnet-implementation-R-squared-values, echo=FALSE, eval=FALSE}

# Compute R-squared
R_squared_relaxed_min <- round(cv_fit_relaxed$glmnet.fit$relaxed$dev.ratio[lambda_min_index], 3)
R_squared_relaxed_1se <- round(cv_fit_relaxed$glmnet.fit$relaxed$dev.ratio[lambda_1se_index], 3)
R_squared_lasso_then_ridge_same_lambda <- round(fit_lasso_then_ridge_same_lambda$dev.ratio, 3)
R_squared_lasso_then_ridge_cv_lambda <- round(fit_lasso_then_ridge_cv_lambda$dev.ratio, 3)
R_squared_lasso_then_OLS_same_lambda <- round(fit_lasso_then_OLS_same_lambda$dev.ratio, 3)
R_squared_lasso_then_OLS_cv_lambda <- round(fit_lasso_then_OLS_cv_lambda$dev.ratio, 3)



R_squared_values = data.frame(matrix(nrow = 6, ncol = 2))
colnames(R_squared_values) = c("Model fitting procedure", "R-squared")
R_squared_values[, 1] = model_fit_labels
R_squared_values[, 2] = c(R_squared_relaxed_min, R_squared_relaxed_1se, R_squared_lasso_then_ridge_same_lambda, R_squared_lasso_then_ridge_cv_lambda, R_squared_lasso_then_OLS_same_lambda, R_squared_lasso_then_OLS_cv_lambda)

R_squared_values
```

## Test MSE values

``` {r relaxed-lasso-glmnet-implementation-AIC-BIC-values, echo=FALSE}
# pred_fit_relaxed_1se = predict(cv_fit_relaxed, newx = x_test, s = lambda_relaxed_1se)
# pred_fit_relaxed_min = predict(cv_fit_relaxed, newx = x_test, s = lambda_relaxed_min)
pred_fit_relaxed_1se = predict(cv_fit_relaxed, newx = x_test, s = cv_lambda_relaxed_1se)
pred_fit_relaxed_min = predict(cv_fit_relaxed, newx = x_test, s = cv_lambda_relaxed_min)

new_x_test = x_test[, non_zero_coef_lasso_then_OLS_all_predictors_indeces_best_fit]
colnames(new_x_test) = names(fit_lasso_then_OLS_all_predictors$coefficients)[-1]
X = as.data.frame.matrix(new_x_test)

pred_fit_lasso_then_OLS_all_predictors = predict(fit_lasso_then_OLS_all_predictors, newdata = X)

new_x_test = x_test[, non_zero_coef_lasso_cv_then_OLS_all_predictors_indeces_best_fit]
colnames(new_x_test) = names(fit_lasso_cv_then_OLS_all_predictors$coefficients)[-1]
X = as.data.frame.matrix(new_x_test)

pred_fit_lasso_cv_then_OLS_all_predictors = predict(fit_lasso_cv_then_OLS_all_predictors, newdata = X)


# Compute MSE
mse_relaxed_min <- sum((pred_fit_relaxed_min - y_test)^2)
mse_relaxed_1se <- sum((pred_fit_relaxed_1se - y_test)^2)
mse_lasso_then_OLS_all_predictors <- sum((pred_fit_lasso_then_OLS_all_predictors - y_test)^2)
mse_lasso_cv_then_OLS_all_predictors <- sum((pred_fit_lasso_cv_then_OLS_all_predictors - y_test)^2)

MSEs = data.frame(matrix(nrow = 4, ncol = 2))
colnames(MSEs) = c("Model fitting procedure", "Test MSE")
MSEs[, 1] = model_fit_labels
MSEs[, 2] = c(mse_relaxed_min, mse_relaxed_1se, mse_lasso_then_OLS_all_predictors, mse_lasso_cv_then_OLS_all_predictors)

MSEs

# # Compute the AIC
# aic <- n*log(rss/n) + 2*k
# 
# # Compute the "BIC"
# bic <- n*log(rss/n) + log(n)*k

```

## CV MSE plot by log lambda

``` {r relaxed-lasso-glmnet-cv-results, echo=FALSE}
par(mfrow = c(1, 1), mar = c(2, 4, 6, 4))
plot(cv_fit_relaxed, main = "relax = TRUE fit, cv lambda")
plot(fit_OLS_cv_on_LASSO_cv_subset, main = "relax = FALSE fit, only lasso, cv lambda")
#plot(fit_lasso_cv_then_OLS_all_predictors, main = "relax = FALSE fit, only lasso, cv lambda")

# plot(fit_lasso_then_ridge_cv_lambda, main = "relax = FALSE fit, lasso then ridge, lambda = 1se lambda found with cv on lasso")
# plot(fit_lasso_then_OLS_same_lambda, main = "relax = FALSE fit, lasso then OLS, lambda = relaxed lambda min")
# plot(fit_lasso_then_OLS_cv_lambda, main = "relax = FALSE fit, lasso then OLS, lambda = 1se lambda found with cv on lasso")
```



