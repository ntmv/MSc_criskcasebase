---
title: "Relaxed Lasso multinomial LR simulation with multinomial data"
author: "Alexander Romanus"
date: "`r Sys.Date()`"
knit: (function(inputFile, encoding) { 
      out_dir <- 'results/knits';
      rmarkdown::render(inputFile,
                        encoding=encoding, 
                        output_file=file.path(dirname(inputFile), out_dir, paste(runif(1), 'relaxed_simulation_multinomial.html'))) })
output:
  html_document: 
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
library(casebase)
library(future.apply)
library(glmnet)
#library(mtool)
library(parallelly)
library(timereg)
library(parallel)
library(tictoc)
library(tidyverse)
#library(riskRegression)
library(cmprsk)
library(survsim)
library(caret)
library(Matrix)
library(dplyr)


# Helper functions 
source("../src/helper_functions.R")
source("../src/fitting_functions.R")
```


# Setup n > p (n = 500)
``` {r setup-simulation,echo=FALSE}
p = 20
n = 400
nfolds = 5
seed = 2023

# Generate covariates 
X <- matrix(rnorm(n * p), n, p)

# coefficients for each choice
X1 <- rep(0, p)
X2 <- c(rep(3, p/2), rep(0, p/2))
zero_X2 <- which(X2 == 0)

X3 <- c(rep(3, p/2), rep(0, p/2))
zero_X3 <- which(X3 == 0)


# vector of probabilities
vProb = cbind(exp(X%*%X1), exp(X%*%X2), exp(X%*%X3))

# multinomial draws
mChoices <- t(apply(vProb, 1, rmultinom, n = 1, size = 1))
dfM <- cbind.data.frame(y = apply(mChoices, 1, function(x) which(x == 1)), X)
# Rename covariates 
colnames(dfM)[2:(p+1)] <- paste0('x', 1:p)

# 0, 1, 2 for levels 
Y <- factor(dfM$y-1)

# Covariate matrix 
X <- as.matrix(dfM[, c(2:(p+1))])

# Rename covariates 
colnames(X) <- paste0('x', 1:p)
```


# Check cv.glmnet(relax=TRUE) vs mtool relaxed implementation on multinomial data

``` {r compare-implementations, echo=FALSE}
alpha <- 0
lambda <- 0.1
lambda1 <- lambda*alpha
lambda2 <- 0.5*lambda*(1 - alpha)

# glmnet relax = TRUE
set.seed(seed)
fit.glmnet.relaxed <- glmnet::cv.glmnet(
  x = X, y = Y,
  family = "multinomial",
  intercept = TRUE,
  type.multinomial = "grouped",  # same sparsity pattern for all outcome classes
  alpha = 1, nfolds = 10,
  relax = TRUE, gamma = 0)
# Elastic-net reparametrization
# mtool
glmnet_lambda_max = fit.glmnet.relaxed$lambda[1]
fit.mtool.relaxed <- multinom.relaxed_enet_nnet(X = X, Y = Y, lambda_max = glmnet_lambda_max, alpha = 1, seed = seed, epsilon = 0.1)

plot(fit.glmnet.relaxed)
plot_post_relaxed.multinom(fit.mtool.relaxed)
```

# Check nnet unparameterized multinomial logit vs mtool with low lambda

``` {r fit-nnet-mtool, echo=FALSE, message = FALSE,results='hide'}
list = paste0('x', 1:p)
formula = ""
for (i in list) {
    formula = paste(formula, i, "+")
}
formula = as.formula(paste("y ~", substr(formula, 2, str_length(formula) - 2), "- 1"))
capture.output(fit.nnet <- nnet::multinom(formula,
                           data = dfM))

cb_data_test = list("event_ind" = as.numeric(Y),
                         "covariates" = X,
                         "offset" = rep(0, length(Y)))

capture.output(fit.mtool.test <- fit_cbmodel(cb_data_test, regularization = 'elastic-net',
                                lambda = 0.000001, alpha = 0.7, unpen_cov = 1))

```

``` {r compare-nnet-mtool, echo=FALSE}
fit.nnet$deviance
multi_deviance_fsh(covs = X, response = Y, fit.mtool.test$coefficients)
```

